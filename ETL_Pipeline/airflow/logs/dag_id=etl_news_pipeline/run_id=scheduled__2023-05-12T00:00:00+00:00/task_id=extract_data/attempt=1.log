[2023-05-15T00:23:26.829+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:23:26.835+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:23:26.836+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T00:23:26.849+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T00:23:26.875+0200] {standard_task_runner.py:57} INFO - Started process 1680 to run task
[2023-05-15T00:23:26.884+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1367', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpjh9i9dji']
[2023-05-15T00:23:26.886+0200] {standard_task_runner.py:85} INFO - Job 1367: Subtask extract_data
[2023-05-15T00:23:26.941+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T00:23:27.007+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T00:23:27.009+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T00:23:27.010+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T00:23:27.031+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T00:23:29.230+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T00:23:29.270+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T222326, end_date=20230514T222329
[2023-05-15T00:23:29.310+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T00:23:29.329+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T00:25:10.304+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:25:10.312+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:25:10.312+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T00:25:10.321+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T00:25:10.351+0200] {standard_task_runner.py:57} INFO - Started process 1836 to run task
[2023-05-15T00:25:10.365+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1369', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpjsdpdk8h']
[2023-05-15T00:25:10.368+0200] {standard_task_runner.py:85} INFO - Job 1369: Subtask extract_data
[2023-05-15T00:25:10.429+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T00:25:10.493+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T00:25:10.494+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T00:25:10.495+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T00:25:10.521+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T00:25:12.721+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T00:25:12.767+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T222510, end_date=20230514T222512
[2023-05-15T00:25:12.794+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T00:25:12.813+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T00:29:25.643+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:29:25.650+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:29:25.651+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T00:29:25.663+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T00:29:25.719+0200] {standard_task_runner.py:57} INFO - Started process 2153 to run task
[2023-05-15T00:29:25.733+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1384', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp7nsj0es1']
[2023-05-15T00:29:25.736+0200] {standard_task_runner.py:85} INFO - Job 1384: Subtask extract_data
[2023-05-15T00:29:25.803+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T00:29:25.898+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T00:29:25.899+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T00:29:25.900+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T00:29:25.929+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T00:29:28.190+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T00:29:28.231+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T222925, end_date=20230514T222928
[2023-05-15T00:29:28.277+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T00:29:28.306+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T00:32:02.594+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:32:02.599+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:32:02.600+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T00:32:02.609+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T00:32:02.632+0200] {standard_task_runner.py:57} INFO - Started process 2373 to run task
[2023-05-15T00:32:02.643+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1387', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmprzhcttvx']
[2023-05-15T00:32:02.646+0200] {standard_task_runner.py:85} INFO - Job 1387: Subtask extract_data
[2023-05-15T00:32:02.698+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T00:32:02.755+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T00:32:02.757+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T00:32:02.757+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T00:32:02.785+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T00:32:05.027+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T00:32:05.065+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T223202, end_date=20230514T223205
[2023-05-15T00:32:05.107+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T00:32:05.132+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T00:36:51.105+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:36:51.111+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:36:51.112+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T00:36:51.128+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T00:36:51.161+0200] {standard_task_runner.py:57} INFO - Started process 2679 to run task
[2023-05-15T00:36:51.170+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1397', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpcibpdbfh']
[2023-05-15T00:36:51.172+0200] {standard_task_runner.py:85} INFO - Job 1397: Subtask extract_data
[2023-05-15T00:36:51.232+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T00:36:51.289+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T00:36:51.291+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T00:36:51.292+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T00:36:51.318+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T00:36:53.600+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T00:36:53.637+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T223651, end_date=20230514T223653
[2023-05-15T00:36:53.679+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T00:36:53.706+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T00:43:15.046+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:43:15.052+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T00:43:15.053+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T00:43:15.061+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T00:43:15.088+0200] {standard_task_runner.py:57} INFO - Started process 3132 to run task
[2023-05-15T00:43:15.097+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1409', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp0jvoadx3']
[2023-05-15T00:43:15.100+0200] {standard_task_runner.py:85} INFO - Job 1409: Subtask extract_data
[2023-05-15T00:43:15.154+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T00:43:15.210+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T00:43:15.211+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T00:43:15.212+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T00:43:15.238+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T00:43:17.500+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T00:43:17.532+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T224315, end_date=20230514T224317
[2023-05-15T00:43:17.565+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T00:43:17.585+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T01:03:18.586+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:03:18.596+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:03:18.598+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T01:03:18.610+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T01:03:18.683+0200] {standard_task_runner.py:57} INFO - Started process 4113 to run task
[2023-05-15T01:03:18.693+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1421', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpwq9qx34h']
[2023-05-15T01:03:18.697+0200] {standard_task_runner.py:85} INFO - Job 1421: Subtask extract_data
[2023-05-15T01:03:18.793+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T01:03:18.888+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T01:03:18.890+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T01:03:18.892+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', <function extract_data at 0x110fcc860>]
[2023-05-15T01:03:18.893+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/hooks/subprocess.py", line 77, in run_command
    self.sub_process = Popen(
                       ^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1024, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1850, in _execute_child
    self.pid = _fork_exec(
               ^^^^^^^^^^^
TypeError: expected str, bytes or os.PathLike object, not function
[2023-05-15T01:03:18.920+0200] {taskinstance.py:1368} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T230318, end_date=20230514T230318
[2023-05-15T01:03:18.951+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1421 for task extract_data (expected str, bytes or os.PathLike object, not function; 4113)
[2023-05-15T01:03:18.990+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-15T01:03:19.017+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T01:09:18.395+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:09:18.404+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:09:18.405+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T01:09:18.424+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T01:09:18.441+0200] {standard_task_runner.py:57} INFO - Started process 4426 to run task
[2023-05-15T01:09:18.449+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1428', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp_d4548mc']
[2023-05-15T01:09:18.454+0200] {standard_task_runner.py:85} INFO - Job 1428: Subtask extract_data
[2023-05-15T01:09:18.526+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T01:09:18.779+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T01:09:18.781+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T01:09:18.782+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T01:09:18.801+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T01:11:50.295+0200] {local_task_job_runner.py:298} WARNING - State of this instance has been externally set to None. Terminating instance.
[2023-05-15T01:11:50.351+0200] {process_utils.py:131} INFO - Sending 15 to group 4426. PIDs of all processes in the group: [4449, 4426]
[2023-05-15T01:11:50.354+0200] {process_utils.py:86} INFO - Sending the signal 15 to group 4426
[2023-05-15T01:11:50.403+0200] {taskinstance.py:1540} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-05-15T01:11:50.568+0200] {subprocess.py:104} INFO - Sending SIGTERM signal to process group
[2023-05-15T01:11:50.799+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/hooks/subprocess.py", line 91, in run_command
    for raw_line in iter(self.sub_process.stdout.readline, b""):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1542, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-05-15T01:11:50.845+0200] {taskinstance.py:1368} INFO - Marking task as FAILED. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T230918, end_date=20230514T231150
[2023-05-15T01:11:50.902+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1428 for task extract_data ((psycopg2.errors.ForeignKeyViolation) insert or update on table "task_fail" violates foreign key constraint "task_fail_ti_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(etl_news_pipeline, extract_data, scheduled__2023-05-12T00:00:00+00:00, -1) is not present in table "task_instance".

[SQL: INSERT INTO task_fail (task_id, dag_id, run_id, map_index, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(run_id)s, %(map_index)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 'extract_data', 'dag_id': 'etl_news_pipeline', 'run_id': 'scheduled__2023-05-12T00:00:00+00:00', 'map_index': -1, 'start_date': datetime.datetime(2023, 5, 14, 23, 9, 18, 396571, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2023, 5, 14, 23, 11, 50, 832345, tzinfo=Timezone('UTC')), 'duration': 152}]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 4426)
[2023-05-15T01:11:50.962+0200] {process_utils.py:79} INFO - Process psutil.Process(pid=4449, status='terminated', started='01:09:18') (4449) terminated with exit code None
[2023-05-15T01:11:50.964+0200] {process_utils.py:79} INFO - Process psutil.Process(pid=4426, status='terminated', exitcode=1, started='01:09:18') (4426) terminated with exit code 1
[2023-05-15T01:14:22.928+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:14:22.937+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:14:22.938+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T01:14:22.952+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T01:14:22.973+0200] {standard_task_runner.py:57} INFO - Started process 4786 to run task
[2023-05-15T01:14:22.983+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1432', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp9caoyrwz']
[2023-05-15T01:14:22.987+0200] {standard_task_runner.py:85} INFO - Job 1432: Subtask extract_data
[2023-05-15T01:14:23.070+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T01:14:23.157+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T01:16:55.074+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:16:55.087+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:16:55.088+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T01:16:55.101+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T01:16:55.124+0200] {standard_task_runner.py:57} INFO - Started process 5091 to run task
[2023-05-15T01:16:55.136+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1436', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpa5zmms48']
[2023-05-15T01:16:55.140+0200] {standard_task_runner.py:85} INFO - Job 1436: Subtask extract_data
[2023-05-15T01:20:02.899+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:20:02.907+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:20:02.908+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T01:20:02.921+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T01:20:02.972+0200] {standard_task_runner.py:57} INFO - Started process 5385 to run task
[2023-05-15T01:20:02.987+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1439', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp19_xxz32']
[2023-05-15T01:20:02.991+0200] {standard_task_runner.py:85} INFO - Job 1439: Subtask extract_data
[2023-05-15T01:20:03.055+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T01:20:03.127+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T01:42:22.498+0200] {local_task_job_runner.py:298} WARNING - State of this instance has been externally set to removed. Terminating instance.
[2023-05-15T01:42:22.561+0200] {process_utils.py:131} INFO - Sending 15 to group 5385. PIDs of all processes in the group: [5385]
[2023-05-15T01:42:22.561+0200] {process_utils.py:86} INFO - Sending the signal 15 to group 5385
[2023-05-15T02:23:11.311+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:23:11.317+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:23:11.318+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T02:23:11.329+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:23:11.353+0200] {standard_task_runner.py:57} INFO - Started process 2211 to run task
[2023-05-15T02:23:11.364+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1495', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp3xd_u2l6']
[2023-05-15T02:23:11.368+0200] {standard_task_runner.py:85} INFO - Job 1495: Subtask extract_data
[2023-05-15T02:23:11.424+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:23:11.495+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T02:23:11.496+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:23:11.497+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T02:23:11.525+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T02:23:14.063+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T02:23:14.100+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T002311, end_date=20230515T002314
[2023-05-15T02:23:14.118+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T02:23:14.140+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:27:37.538+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:27:37.544+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:27:37.545+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T02:27:37.553+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:27:37.581+0200] {standard_task_runner.py:57} INFO - Started process 2490 to run task
[2023-05-15T02:27:37.591+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1504', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpijhp2lkv']
[2023-05-15T02:27:37.594+0200] {standard_task_runner.py:85} INFO - Job 1504: Subtask extract_data
[2023-05-15T02:27:37.655+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:27:37.727+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T02:42:35.088+0200] {process_utils.py:131} INFO - Sending 15 to group 2490. PIDs of all processes in the group: [2490]
[2023-05-15T02:42:35.097+0200] {process_utils.py:86} INFO - Sending the signal 15 to group 2490
[2023-05-15T02:43:55.959+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:43:55.973+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:43:55.974+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T02:43:55.987+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:43:56.009+0200] {standard_task_runner.py:57} INFO - Started process 4428 to run task
[2023-05-15T02:43:56.019+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1511', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpcfvyovgo']
[2023-05-15T02:43:56.024+0200] {standard_task_runner.py:85} INFO - Job 1511: Subtask extract_data
[2023-05-15T02:43:56.080+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:43:56.158+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T02:43:56.159+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:43:56.160+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', True]
[2023-05-15T02:43:56.161+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/hooks/subprocess.py", line 77, in run_command
    self.sub_process = Popen(
                       ^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1024, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1850, in _execute_child
    self.pid = _fork_exec(
               ^^^^^^^^^^^
TypeError: expected str, bytes or os.PathLike object, not bool
[2023-05-15T02:43:56.172+0200] {taskinstance.py:1368} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T004355, end_date=20230515T004356
[2023-05-15T02:43:56.182+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1511 for task extract_data (expected str, bytes or os.PathLike object, not bool; 4428)
[2023-05-15T02:43:56.202+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-15T02:43:56.233+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:47:01.378+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:47:01.386+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:47:01.386+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T02:47:01.396+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:47:01.464+0200] {standard_task_runner.py:57} INFO - Started process 4638 to run task
[2023-05-15T02:47:01.476+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1513', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmps0xcln2_']
[2023-05-15T02:47:01.481+0200] {standard_task_runner.py:85} INFO - Job 1513: Subtask extract_data
[2023-05-15T02:47:01.561+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:47:01.650+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T02:47:01.651+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:47:01.652+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', <function extract_data at 0x115dcc860>]
[2023-05-15T02:47:01.655+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/hooks/subprocess.py", line 77, in run_command
    self.sub_process = Popen(
                       ^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1024, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1850, in _execute_child
    self.pid = _fork_exec(
               ^^^^^^^^^^^
TypeError: expected str, bytes or os.PathLike object, not function
[2023-05-15T02:47:01.670+0200] {taskinstance.py:1368} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T004701, end_date=20230515T004701
[2023-05-15T02:47:01.684+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1513 for task extract_data (expected str, bytes or os.PathLike object, not function; 4638)
[2023-05-15T02:47:01.730+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-15T02:47:01.763+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:48:40.146+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:48:40.156+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:48:40.157+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T02:48:40.173+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:48:40.246+0200] {standard_task_runner.py:57} INFO - Started process 4817 to run task
[2023-05-15T02:48:40.263+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1521', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpt0n8zl3p']
[2023-05-15T02:48:40.273+0200] {standard_task_runner.py:85} INFO - Job 1521: Subtask extract_data
[2023-05-15T02:48:40.363+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:48:40.464+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T02:48:40.466+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:48:40.468+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T02:48:40.511+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T02:48:45.315+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T02:48:45.349+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T004840, end_date=20230515T004845
[2023-05-15T02:48:45.404+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T02:48:45.429+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:53:24.325+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:53:24.339+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:53:24.340+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T02:53:24.356+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:53:24.395+0200] {standard_task_runner.py:57} INFO - Started process 5148 to run task
[2023-05-15T02:53:24.413+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1526', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpsm4mt7gr']
[2023-05-15T02:53:24.416+0200] {standard_task_runner.py:85} INFO - Job 1526: Subtask extract_data
[2023-05-15T02:53:24.478+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:53:24.557+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T02:53:24.559+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:53:24.560+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python -c extract_data']
[2023-05-15T02:53:24.602+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T02:53:24.655+0200] {subprocess.py:93} INFO - Traceback (most recent call last):
[2023-05-15T02:53:24.655+0200] {subprocess.py:93} INFO -   File "<string>", line 1, in <module>
[2023-05-15T02:53:24.655+0200] {subprocess.py:93} INFO - NameError: name 'extract_data' is not defined
[2023-05-15T02:53:24.658+0200] {subprocess.py:97} INFO - Command exited with return code 1
[2023-05-15T02:53:24.674+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2023-05-15T02:53:24.682+0200] {taskinstance.py:1368} INFO - Marking task as UP_FOR_RETRY. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T005324, end_date=20230515T005324
[2023-05-15T02:53:24.703+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1526 for task extract_data (Bash command failed. The command returned a non-zero exit code 1.; 5148)
[2023-05-15T02:53:24.721+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-15T02:53:24.750+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:57:21.949+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:57:21.959+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:57:21.960+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T02:57:21.974+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:57:22.058+0200] {standard_task_runner.py:57} INFO - Started process 5434 to run task
[2023-05-15T02:57:22.069+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1536', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpk2erzl55']
[2023-05-15T02:57:22.074+0200] {standard_task_runner.py:85} INFO - Job 1536: Subtask extract_data
[2023-05-15T02:57:22.150+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:57:22.248+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T02:57:22.251+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:57:22.252+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T02:57:22.287+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T02:57:31.062+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T02:57:31.094+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T005721, end_date=20230515T005731
[2023-05-15T02:57:31.141+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T02:57:31.161+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T03:04:49.157+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:04:49.167+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:04:49.168+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T03:04:49.182+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T03:04:49.197+0200] {standard_task_runner.py:57} INFO - Started process 5794 to run task
[2023-05-15T03:04:49.216+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1547', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpkn99uva4']
[2023-05-15T03:04:49.229+0200] {standard_task_runner.py:85} INFO - Job 1547: Subtask extract_data
[2023-05-15T03:04:49.296+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T03:04:49.640+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T03:04:49.651+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T03:04:49.664+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py get_data']
[2023-05-15T03:04:49.684+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T03:04:57.487+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T03:04:57.527+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T010449, end_date=20230515T010457
[2023-05-15T03:04:57.582+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T03:04:57.611+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T03:16:56.344+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:16:56.352+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:16:56.352+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T03:16:56.361+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T03:16:56.383+0200] {standard_task_runner.py:57} INFO - Started process 6238 to run task
[2023-05-15T03:16:56.395+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1553', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpwqeryy93']
[2023-05-15T03:16:56.397+0200] {standard_task_runner.py:85} INFO - Job 1553: Subtask extract_data
[2023-05-15T03:16:56.456+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T03:16:56.530+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T03:16:56.534+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T03:16:56.534+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T03:16:56.559+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T03:25:57.370+0200] {subprocess.py:93} INFO -                  Date  ...                                            Summary
[2023-05-15T03:25:57.393+0200] {subprocess.py:93} INFO - 0 2023-05-15 01:18:10  ...  Isaac River coalmine project met the standards...
[2023-05-15T03:25:57.394+0200] {subprocess.py:93} INFO - 
[2023-05-15T03:25:57.395+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-15T03:25:59.249+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T03:25:59.353+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T011656, end_date=20230515T012559
[2023-05-15T03:25:59.405+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T03:25:59.488+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T03:29:28.599+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:29:28.611+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:29:28.611+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T03:29:28.620+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T03:29:28.637+0200] {standard_task_runner.py:57} INFO - Started process 8137 to run task
[2023-05-15T03:29:28.642+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1563', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp8726xnle']
[2023-05-15T03:29:28.645+0200] {standard_task_runner.py:85} INFO - Job 1563: Subtask extract_data
[2023-05-15T03:29:28.706+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T03:29:29.117+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T03:29:29.119+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T03:29:29.121+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T03:29:29.136+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T03:35:25.204+0200] {subprocess.py:93} INFO -                  Date  ...                                            Summary
[2023-05-15T03:35:25.237+0200] {subprocess.py:93} INFO - 0 2023-05-15 01:29:31  ...  Newly approved coalmine is a ‘small project’ p...
[2023-05-15T03:35:25.240+0200] {subprocess.py:93} INFO - 
[2023-05-15T03:35:25.242+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-15T03:35:27.257+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T03:35:27.398+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T012928, end_date=20230515T013527
[2023-05-15T03:35:27.459+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T03:35:27.547+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T03:39:30.276+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:39:30.284+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:39:30.284+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T03:39:30.296+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T03:39:30.310+0200] {standard_task_runner.py:57} INFO - Started process 9135 to run task
[2023-05-15T03:39:30.316+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1574', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpaouwqokp']
[2023-05-15T03:39:30.319+0200] {standard_task_runner.py:85} INFO - Job 1574: Subtask extract_data
[2023-05-15T03:39:30.375+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T03:39:30.641+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T03:39:30.643+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T03:39:30.644+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T03:39:30.661+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T03:44:55.383+0200] {subprocess.py:93} INFO -                  Date  ...                                            Summary
[2023-05-15T03:44:55.411+0200] {subprocess.py:93} INFO - 0 2023-05-15 01:35:00  ...  Newly approved coalmine is a ‘small project’ p...
[2023-05-15T03:44:55.414+0200] {subprocess.py:93} INFO - 
[2023-05-15T03:44:55.416+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-15T03:44:57.032+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T03:44:57.172+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T013930, end_date=20230515T014457
[2023-05-15T03:44:57.227+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T03:44:57.334+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T03:47:41.480+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:47:41.497+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T03:47:41.499+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T03:47:41.512+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T03:47:41.526+0200] {standard_task_runner.py:57} INFO - Started process 10057 to run task
[2023-05-15T03:47:41.531+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1582', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpay7woayk']
[2023-05-15T03:47:41.535+0200] {standard_task_runner.py:85} INFO - Job 1582: Subtask extract_data
[2023-05-15T03:47:41.594+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T03:47:41.954+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T03:47:41.955+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T03:47:41.955+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T03:47:41.970+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T03:56:44.497+0200] {subprocess.py:93} INFO -                  Date  ...                                            Summary
[2023-05-15T03:56:44.523+0200] {subprocess.py:93} INFO - 0 2023-05-15 01:47:52  ...  Turkish president Recep Tayyip Erdoan has just...
[2023-05-15T03:56:44.524+0200] {subprocess.py:93} INFO - 
[2023-05-15T03:56:44.527+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-15T03:56:46.254+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T03:56:46.358+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T014741, end_date=20230515T015646
[2023-05-15T03:56:46.420+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T03:56:46.521+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T04:22:11.969+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T04:22:11.982+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T04:22:11.983+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T04:22:11.999+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T04:22:12.016+0200] {standard_task_runner.py:57} INFO - Started process 14155 to run task
[2023-05-15T04:22:12.024+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1596', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp7cdceckx']
[2023-05-15T04:22:12.034+0200] {standard_task_runner.py:85} INFO - Job 1596: Subtask extract_data
[2023-05-15T04:22:12.110+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T04:22:12.528+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T04:22:12.529+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T04:22:12.529+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T04:22:12.544+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T04:28:44.330+0200] {subprocess.py:93} INFO -                  Date  ...                                            Summary
[2023-05-15T04:28:44.359+0200] {subprocess.py:93} INFO - 0 2023-05-15 02:19:02  ...  NAB predicts further RBA cash rate hike by Jul...
[2023-05-15T04:28:44.361+0200] {subprocess.py:93} INFO - 
[2023-05-15T04:28:44.361+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-15T04:28:45.409+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T04:28:45.537+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T022211, end_date=20230515T022845
[2023-05-15T04:28:45.610+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T04:28:45.687+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T07:31:36.744+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T07:31:36.755+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T07:31:36.755+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T07:31:36.765+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T07:31:36.781+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1619', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpt4pa1gu3']
[2023-05-15T07:31:36.784+0200] {standard_task_runner.py:85} INFO - Job 1619: Subtask extract_data
[2023-05-15T07:31:36.835+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T07:31:36.776+0200] {standard_task_runner.py:57} INFO - Started process 18430 to run task
[2023-05-15T07:31:37.371+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T07:31:37.375+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T07:31:37.376+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T07:31:37.395+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T07:37:45.392+0200] {subprocess.py:93} INFO -                  Date  ...                                            Summary
[2023-05-15T07:37:45.418+0200] {subprocess.py:93} INFO - 0 2023-05-15 05:27:35  ...  PwC Australia has announced Ziggy Switkowski w...
[2023-05-15T07:37:45.421+0200] {subprocess.py:93} INFO - 
[2023-05-15T07:37:45.422+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-15T07:37:46.778+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T07:37:46.900+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T053136, end_date=20230515T053746
[2023-05-15T07:37:46.967+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T07:37:47.103+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T08:33:34.510+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T08:33:34.517+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T08:33:34.518+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-15T08:33:34.534+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T08:33:34.558+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1647', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpbqw2oagt']
[2023-05-15T08:33:34.562+0200] {standard_task_runner.py:85} INFO - Job 1647: Subtask extract_data
[2023-05-15T08:33:34.612+0200] {standard_task_runner.py:57} INFO - Started process 24264 to run task
[2023-05-15T08:33:34.623+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T08:33:35.207+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T08:33:35.208+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T08:33:35.209+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T08:33:35.227+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T08:40:03.094+0200] {subprocess.py:93} INFO -                  Date  ...                                            Summary
[2023-05-15T08:40:03.119+0200] {subprocess.py:93} INFO - 0 2023-05-15 06:24:56  ...  Volodymyr Zelenskiy will arrive in London on M...
[2023-05-15T08:40:03.120+0200] {subprocess.py:93} INFO - 
[2023-05-15T08:40:03.123+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-15T08:40:04.675+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T08:40:04.853+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T063334, end_date=20230515T064004
[2023-05-15T08:40:04.909+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T08:40:05.082+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
