[2023-05-15T01:00:14.692+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:00:14.699+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:00:14.699+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 2
[2023-05-15T01:00:14.709+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T01:00:14.729+0200] {standard_task_runner.py:57} INFO - Started process 3889 to run task
[2023-05-15T01:00:14.740+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1418', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpxb0_o_7q']
[2023-05-15T01:00:14.743+0200] {standard_task_runner.py:85} INFO - Job 1418: Subtask extract_data
[2023-05-15T01:00:14.840+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T01:00:15.029+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T01:01:15.198+0200] {process_utils.py:131} INFO - Sending 15 to group 3889. PIDs of all processes in the group: [3889]
[2023-05-15T01:01:15.200+0200] {process_utils.py:86} INFO - Sending the signal 15 to group 3889
[2023-05-15T01:05:28.795+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:05:28.804+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T01:05:28.805+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 2
[2023-05-15T01:05:28.819+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T01:05:28.874+0200] {standard_task_runner.py:57} INFO - Started process 4211 to run task
[2023-05-15T01:05:28.913+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1423', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpjl3fpm76']
[2023-05-15T01:05:28.918+0200] {standard_task_runner.py:85} INFO - Job 1423: Subtask extract_data
[2023-05-15T01:05:29.013+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T01:05:29.101+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T01:05:29.102+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T01:05:29.103+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', <function extract_data at 0x111574860>]
[2023-05-15T01:05:29.105+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/hooks/subprocess.py", line 77, in run_command
    self.sub_process = Popen(
                       ^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1024, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1850, in _execute_child
    self.pid = _fork_exec(
               ^^^^^^^^^^^
TypeError: expected str, bytes or os.PathLike object, not function
[2023-05-15T01:05:29.119+0200] {taskinstance.py:1368} INFO - Marking task as FAILED. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230514T230528, end_date=20230514T230529
[2023-05-15T01:05:29.134+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1423 for task extract_data (expected str, bytes or os.PathLike object, not function; 4211)
[2023-05-15T01:05:29.153+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-15T01:05:29.173+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:18:50.547+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:18:50.553+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:18:50.553+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 2
[2023-05-15T02:18:50.561+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:18:50.583+0200] {standard_task_runner.py:57} INFO - Started process 1924 to run task
[2023-05-15T02:18:50.592+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1487', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpwo_4y_ys']
[2023-05-15T02:18:50.595+0200] {standard_task_runner.py:85} INFO - Job 1487: Subtask extract_data
[2023-05-15T02:18:50.650+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:18:50.717+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T02:18:50.718+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:18:50.719+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T02:18:50.745+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T02:18:53.067+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T02:18:53.106+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230512T000000, start_date=20230515T001850, end_date=20230515T001853
[2023-05-15T02:18:53.143+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T02:18:53.168+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:55:39.054+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:55:39.063+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T02:55:39.064+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 2
[2023-05-15T02:55:39.084+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T02:55:39.128+0200] {standard_task_runner.py:57} INFO - Started process 5239 to run task
[2023-05-15T02:55:39.139+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1530', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpoysm8lff']
[2023-05-15T02:55:39.151+0200] {standard_task_runner.py:85} INFO - Job 1530: Subtask extract_data
[2023-05-15T02:56:29.586+0200] {process_utils.py:131} INFO - Sending 15 to group 5239. PIDs of all processes in the group: [5239]
[2023-05-15T02:56:29.587+0200] {process_utils.py:86} INFO - Sending the signal 15 to group 5239
[2023-05-15T02:56:29.595+0200] {process_utils.py:79} INFO - Process psutil.Process(pid=5239, status='terminated', exitcode=<Negsignal.SIGTERM: -15>, started='02:55:39') (5239) terminated with exit code -15
[2023-05-15T04:18:32.494+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T04:18:32.501+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [queued]>
[2023-05-15T04:18:32.501+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 3
[2023-05-15T04:18:32.511+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-12 00:00:00+00:00
[2023-05-15T04:18:32.524+0200] {standard_task_runner.py:57} INFO - Started process 13684 to run task
[2023-05-15T04:18:32.531+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-12T00:00:00+00:00', '--job-id', '1589', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp6_tnsfto']
[2023-05-15T04:18:32.534+0200] {standard_task_runner.py:85} INFO - Job 1589: Subtask extract_data
[2023-05-15T04:18:32.593+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-12T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T04:18:32.806+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-12T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-12T00:00:00+00:00'
[2023-05-15T04:18:32.807+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T04:18:32.807+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-15T04:18:32.822+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T04:18:42.636+0200] {local_task_job_runner.py:298} WARNING - State of this instance has been externally set to skipped. Terminating instance.
[2023-05-15T04:18:42.646+0200] {process_utils.py:131} INFO - Sending 15 to group 13684. PIDs of all processes in the group: [13692, 13684]
[2023-05-15T04:18:42.646+0200] {process_utils.py:86} INFO - Sending the signal 15 to group 13684
[2023-05-15T04:18:42.648+0200] {taskinstance.py:1540} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-05-15T04:18:42.649+0200] {subprocess.py:104} INFO - Sending SIGTERM signal to process group
[2023-05-15T04:18:43.715+0200] {process_utils.py:79} INFO - Process psutil.Process(pid=13692, status='terminated', started='04:18:32') (13692) terminated with exit code None
[2023-05-15T04:18:43.718+0200] {process_utils.py:79} INFO - Process psutil.Process(pid=13684, status='terminated', exitcode=0, started='04:18:32') (13684) terminated with exit code 0
