[2023-05-15T00:27:12.637+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [queued]>
[2023-05-15T00:27:12.646+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [queued]>
[2023-05-15T00:27:12.647+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 2
[2023-05-15T00:27:12.659+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-13 00:00:00+00:00
[2023-05-15T00:27:12.682+0200] {standard_task_runner.py:57} INFO - Started process 1962 to run task
[2023-05-15T00:27:12.690+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-13T00:00:00+00:00', '--job-id', '1374', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmprvjqyy6z']
[2023-05-15T00:27:12.692+0200] {standard_task_runner.py:85} INFO - Job 1374: Subtask extract_data
[2023-05-15T00:27:12.749+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T00:27:12.831+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-13T00:00:00+00:00'
[2023-05-15T00:27:12.832+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T00:27:12.832+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T00:27:12.854+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T00:27:15.614+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T00:27:15.647+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230513T000000, start_date=20230514T222712, end_date=20230514T222715
[2023-05-15T00:27:15.690+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T00:27:15.713+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T01:05:28.825+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [queued]>
[2023-05-15T01:05:28.838+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [queued]>
[2023-05-15T01:05:28.839+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 2
[2023-05-15T01:05:28.864+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-13 00:00:00+00:00
[2023-05-15T01:05:28.916+0200] {standard_task_runner.py:57} INFO - Started process 4213 to run task
[2023-05-15T01:05:28.939+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-13T00:00:00+00:00', '--job-id', '1425', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmphpygktgk']
[2023-05-15T01:05:28.945+0200] {standard_task_runner.py:85} INFO - Job 1425: Subtask extract_data
[2023-05-15T01:05:29.025+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T01:05:29.113+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-13T00:00:00+00:00'
[2023-05-15T01:05:29.117+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T01:05:29.119+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', <function extract_data at 0x111604860>]
[2023-05-15T01:05:29.120+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/hooks/subprocess.py", line 77, in run_command
    self.sub_process = Popen(
                       ^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1024, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/subprocess.py", line 1850, in _execute_child
    self.pid = _fork_exec(
               ^^^^^^^^^^^
TypeError: expected str, bytes or os.PathLike object, not function
[2023-05-15T01:05:29.131+0200] {taskinstance.py:1368} INFO - Marking task as FAILED. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230513T000000, start_date=20230514T230528, end_date=20230514T230529
[2023-05-15T01:05:29.142+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1425 for task extract_data (expected str, bytes or os.PathLike object, not function; 4213)
[2023-05-15T01:05:29.183+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-15T01:05:29.210+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:18:50.547+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [queued]>
[2023-05-15T02:18:50.552+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [queued]>
[2023-05-15T02:18:50.553+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 2
[2023-05-15T02:18:50.564+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-13 00:00:00+00:00
[2023-05-15T02:18:50.585+0200] {standard_task_runner.py:57} INFO - Started process 1925 to run task
[2023-05-15T02:18:50.594+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-13T00:00:00+00:00', '--job-id', '1486', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpzqp3gbct']
[2023-05-15T02:18:50.597+0200] {standard_task_runner.py:85} INFO - Job 1486: Subtask extract_data
[2023-05-15T02:18:50.655+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:18:50.723+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-13T00:00:00+00:00'
[2023-05-15T02:18:50.724+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:18:50.725+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py']
[2023-05-15T02:18:50.747+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T02:18:53.069+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-15T02:18:53.108+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230513T000000, start_date=20230515T001850, end_date=20230515T001853
[2023-05-15T02:18:53.139+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-15T02:18:53.164+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-15T02:55:41.126+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [queued]>
[2023-05-15T02:55:41.133+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [queued]>
[2023-05-15T02:55:41.133+0200] {taskinstance.py:1331} INFO - Starting attempt 2 of 2
[2023-05-15T02:55:41.146+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-13 00:00:00+00:00
[2023-05-15T02:55:41.168+0200] {standard_task_runner.py:57} INFO - Started process 5263 to run task
[2023-05-15T02:55:41.175+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-13T00:00:00+00:00', '--job-id', '1532', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpoocfbyf1']
[2023-05-15T02:55:41.177+0200] {standard_task_runner.py:85} INFO - Job 1532: Subtask extract_data
[2023-05-15T02:55:41.245+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-13T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-15T02:55:41.333+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-13T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-13T00:00:00+00:00'
[2023-05-15T02:55:41.335+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-15T02:55:41.336+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python -c extract_data']
[2023-05-15T02:55:41.362+0200] {subprocess.py:86} INFO - Output:
[2023-05-15T02:55:41.419+0200] {subprocess.py:93} INFO - Traceback (most recent call last):
[2023-05-15T02:55:41.420+0200] {subprocess.py:93} INFO -   File "<string>", line 1, in <module>
[2023-05-15T02:55:41.420+0200] {subprocess.py:93} INFO - NameError: name 'extract_data' is not defined
[2023-05-15T02:55:41.426+0200] {subprocess.py:97} INFO - Command exited with return code 1
[2023-05-15T02:55:41.443+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2023-05-15T02:55:41.451+0200] {taskinstance.py:1368} INFO - Marking task as FAILED. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230513T000000, start_date=20230515T005541, end_date=20230515T005541
[2023-05-15T02:55:41.470+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1532 for task extract_data (Bash command failed. The command returned a non-zero exit code 1.; 5263)
[2023-05-15T02:55:41.516+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-15T02:55:41.552+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
