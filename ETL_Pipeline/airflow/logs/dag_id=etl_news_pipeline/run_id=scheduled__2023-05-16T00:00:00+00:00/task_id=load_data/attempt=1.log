[2023-05-17T12:34:24.790+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:34:24.796+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:34:24.796+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T12:34:24.804+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): load_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T12:34:24.819+0200] {standard_task_runner.py:57} INFO - Started process 1617 to run task
[2023-05-17T12:34:24.826+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'load_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1789', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp74a7ilrl']
[2023-05-17T12:34:24.829+0200] {standard_task_runner.py:85} INFO - Job 1789: Subtask load_data
[2023-05-17T12:34:24.889+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T12:34:24.965+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='load_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T12:34:24.966+0200] {postgres.py:158} INFO - Running copy expert: COPY news_etl_table FROM STDIN, filename: /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/data.csv
[2023-05-17T12:34:24.980+0200] {base.py:73} INFO - Using connection ID '***_connection' for task execution.
[2023-05-17T12:34:24.990+0200] {python.py:183} INFO - Done. Returned value was: True
[2023-05-17T12:34:25.013+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=load_data, execution_date=20230516T000000, start_date=20230517T103424, end_date=20230517T103425
[2023-05-17T12:34:25.042+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T12:34:25.063+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-17T12:48:45.633+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:48:45.639+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:48:45.640+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T12:48:45.649+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): load_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T12:48:45.667+0200] {standard_task_runner.py:57} INFO - Started process 2181 to run task
[2023-05-17T12:48:45.675+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'load_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1799', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpdgndz0y0']
[2023-05-17T12:48:45.678+0200] {standard_task_runner.py:85} INFO - Job 1799: Subtask load_data
[2023-05-17T12:51:01.996+0200] {local_task_job_runner.py:298} WARNING - State of this instance has been externally set to None. Terminating instance.
[2023-05-17T12:51:02.013+0200] {process_utils.py:131} INFO - Sending 15 to group 2181. PIDs of all processes in the group: [2181]
[2023-05-17T12:51:02.014+0200] {process_utils.py:86} INFO - Sending the signal 15 to group 2181
[2023-05-17T12:51:02.023+0200] {process_utils.py:79} INFO - Process psutil.Process(pid=2181, status='terminated', exitcode=<Negsignal.SIGTERM: -15>, started='12:48:45') (2181) terminated with exit code -15
[2023-05-17T13:42:52.873+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:42:52.880+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:42:52.881+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T13:42:52.889+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): load_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T13:42:52.917+0200] {standard_task_runner.py:57} INFO - Started process 4677 to run task
[2023-05-17T13:42:52.927+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'load_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1892', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp610t_ge8']
[2023-05-17T13:42:52.931+0200] {standard_task_runner.py:85} INFO - Job 1892: Subtask load_data
[2023-05-17T13:42:52.995+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T13:42:53.057+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='load_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T13:42:53.059+0200] {postgres.py:158} INFO - Running copy expert: COPY news_etl FROM STDIN, filename: /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/data.csv
[2023-05-17T13:42:53.071+0200] {base.py:73} INFO - Using connection ID '***_connection' for task execution.
[2023-05-17T13:42:53.082+0200] {python.py:183} INFO - Done. Returned value was: True
[2023-05-17T13:42:53.101+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=load_data, execution_date=20230516T000000, start_date=20230517T114252, end_date=20230517T114253
[2023-05-17T13:42:53.140+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T13:42:53.162+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-17T13:53:58.187+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:53:58.194+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:53:58.195+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T13:53:58.209+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): load_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T13:53:58.228+0200] {standard_task_runner.py:57} INFO - Started process 6005 to run task
[2023-05-17T13:53:58.236+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'load_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1916', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpknso6u4s']
[2023-05-17T13:53:58.238+0200] {standard_task_runner.py:85} INFO - Job 1916: Subtask load_data
[2023-05-17T13:53:58.291+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T13:53:58.341+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='load_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T13:53:58.342+0200] {postgres.py:158} INFO - Running copy expert: COPY news_etl FROM STDIN, filename: /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/data.csv
[2023-05-17T13:53:58.347+0200] {base.py:73} INFO - Using connection ID '***_connection' for task execution.
[2023-05-17T13:53:58.351+0200] {python.py:183} INFO - Done. Returned value was: True
[2023-05-17T13:53:58.366+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=load_data, execution_date=20230516T000000, start_date=20230517T115358, end_date=20230517T115358
[2023-05-17T13:53:58.409+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T13:53:58.427+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-17T14:01:18.540+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T14:01:18.546+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T14:01:18.546+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T14:01:18.555+0200] {taskinstance.py:1350} INFO - Executing <Task(PythonOperator): load_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T14:01:18.574+0200] {standard_task_runner.py:57} INFO - Started process 6889 to run task
[2023-05-17T14:01:18.582+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'load_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1927', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp235cytc_']
[2023-05-17T14:01:18.584+0200] {standard_task_runner.py:85} INFO - Job 1927: Subtask load_data
[2023-05-17T14:01:18.641+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.load_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T14:01:18.692+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='load_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T14:01:18.693+0200] {postgres.py:158} INFO - Running copy expert: COPY news_etl FROM STDIN, filename: /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/data.csv
[2023-05-17T14:01:18.701+0200] {base.py:73} INFO - Using connection ID '***_connection' for task execution.
[2023-05-17T14:01:18.706+0200] {python.py:183} INFO - Done. Returned value was: True
[2023-05-17T14:01:18.721+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=load_data, execution_date=20230516T000000, start_date=20230517T120118, end_date=20230517T120118
[2023-05-17T14:01:18.755+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T14:01:18.775+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
