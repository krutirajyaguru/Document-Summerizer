[2023-05-17T12:30:37.219+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:30:37.226+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:30:37.226+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T12:30:37.234+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T12:30:37.259+0200] {standard_task_runner.py:57} INFO - Started process 1485 to run task
[2023-05-17T12:30:37.268+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1786', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp6idm19rj']
[2023-05-17T12:30:37.272+0200] {standard_task_runner.py:85} INFO - Job 1786: Subtask extract_data
[2023-05-17T12:30:37.335+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T12:30:37.441+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T12:30:37.442+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T12:30:37.443+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T12:30:37.481+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T12:31:10.433+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T12:31:10.434+0200] {subprocess.py:93} INFO - 0 2023-05-17 10:27:04  ...  Hunt says the government will open borders for...
[2023-05-17T12:31:10.435+0200] {subprocess.py:93} INFO - 
[2023-05-17T12:31:10.435+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T12:31:10.760+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-17T12:31:10.773+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2377, in xcom_push
    XCom.set(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/models/xcom.py", line 214, in set
    raise ValueError(f"DAG run not found on DAG {dag_id!r} with ID {run_id!r}")
ValueError: DAG run not found on DAG 'etl_news_pipeline' with ID 'scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T12:31:10.797+0200] {taskinstance.py:1368} INFO - Marking task as FAILED. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230516T000000, start_date=20230517T103037, end_date=20230517T103110
[2023-05-17T12:31:10.814+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1786 for task extract_data ((psycopg2.errors.ForeignKeyViolation) insert or update on table "task_fail" violates foreign key constraint "task_fail_ti_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(etl_news_pipeline, extract_data, scheduled__2023-05-16T00:00:00+00:00, -1) is not present in table "task_instance".

[SQL: INSERT INTO task_fail (task_id, dag_id, run_id, map_index, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(run_id)s, %(map_index)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 'extract_data', 'dag_id': 'etl_news_pipeline', 'run_id': 'scheduled__2023-05-16T00:00:00+00:00', 'map_index': -1, 'start_date': datetime.datetime(2023, 5, 17, 10, 30, 37, 219710, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2023, 5, 17, 10, 31, 10, 797040, tzinfo=Timezone('UTC')), 'duration': 33}]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 1485)
[2023-05-17T12:31:10.843+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-17T12:33:49.309+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:33:49.315+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:33:49.315+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T12:33:49.324+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T12:33:49.339+0200] {standard_task_runner.py:57} INFO - Started process 1567 to run task
[2023-05-17T12:33:49.346+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1787', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp1bx3567_']
[2023-05-17T12:33:49.348+0200] {standard_task_runner.py:85} INFO - Job 1787: Subtask extract_data
[2023-05-17T12:33:49.406+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T12:33:49.485+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T12:33:49.486+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T12:33:49.487+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T12:33:49.506+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T12:34:15.046+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T12:34:15.049+0200] {subprocess.py:93} INFO - 0 2023-05-17 10:32:49  ...  Hunt says the government will open borders for...
[2023-05-17T12:34:15.049+0200] {subprocess.py:93} INFO - 
[2023-05-17T12:34:15.050+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T12:34:15.359+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-17T12:34:15.406+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230516T000000, start_date=20230517T103349, end_date=20230517T103415
[2023-05-17T12:34:15.435+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T12:34:15.459+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-17T12:45:55.677+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:45:55.683+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:45:55.683+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T12:45:55.691+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T12:45:55.709+0200] {standard_task_runner.py:57} INFO - Started process 1974 to run task
[2023-05-17T12:45:55.718+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1795', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpcafa0fjv']
[2023-05-17T12:45:55.720+0200] {standard_task_runner.py:85} INFO - Job 1795: Subtask extract_data
[2023-05-17T12:45:55.770+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T12:45:55.833+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T12:45:55.835+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T12:45:55.835+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T12:45:55.856+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T12:48:34.718+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T12:48:34.721+0200] {subprocess.py:93} INFO - 0 2023-05-17 10:44:02  ...  Starmer said Labour would give councils and re...
[2023-05-17T12:48:34.721+0200] {subprocess.py:93} INFO - 
[2023-05-17T12:48:34.721+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T12:48:34.722+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T12:48:34.722+0200] {subprocess.py:93} INFO - 0 2023-05-17 10:36:29  ...  Hunt says the government will open borders for...
[2023-05-17T12:48:34.722+0200] {subprocess.py:93} INFO - 
[2023-05-17T12:48:34.722+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T12:48:34.723+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T12:48:34.723+0200] {subprocess.py:93} INFO - 0 2023-05-17 10:00:27  ...  Ex-prosecutors express criticism as key Republ...
[2023-05-17T12:48:34.723+0200] {subprocess.py:93} INFO - 
[2023-05-17T12:48:34.723+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T12:48:34.724+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T12:48:34.724+0200] {subprocess.py:93} INFO - 0 2023-05-17 10:00:26  ...  UN agency says El Nio and human-induced climat...
[2023-05-17T12:48:34.724+0200] {subprocess.py:93} INFO - 
[2023-05-17T12:48:34.724+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T12:48:34.725+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T12:48:34.725+0200] {subprocess.py:93} INFO - 0 2023-05-17 09:58:24  ...  Anthony Albanese has confirmed the Sydney Quad...
[2023-05-17T12:48:34.725+0200] {subprocess.py:93} INFO - 
[2023-05-17T12:48:34.725+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T12:48:35.189+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-17T12:48:35.247+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230516T000000, start_date=20230517T104555, end_date=20230517T104835
[2023-05-17T12:48:35.290+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T12:48:35.329+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-17T12:53:31.429+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:53:31.435+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T12:53:31.436+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T12:53:31.443+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T12:53:31.460+0200] {standard_task_runner.py:57} INFO - Started process 2387 to run task
[2023-05-17T12:53:31.467+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1802', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp9b4grfh0']
[2023-05-17T12:53:31.470+0200] {standard_task_runner.py:85} INFO - Job 1802: Subtask extract_data
[2023-05-17T12:53:31.528+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T12:53:31.613+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T12:53:31.614+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T12:53:31.614+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T12:53:31.635+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T12:57:33.403+0200] {local_task_job_runner.py:298} WARNING - State of this instance has been externally set to None. Terminating instance.
[2023-05-17T12:57:33.414+0200] {process_utils.py:131} INFO - Sending 15 to group 2387. PIDs of all processes in the group: [2395, 2387]
[2023-05-17T12:57:33.415+0200] {process_utils.py:86} INFO - Sending the signal 15 to group 2387
[2023-05-17T12:57:33.417+0200] {taskinstance.py:1540} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-05-17T12:57:33.419+0200] {subprocess.py:104} INFO - Sending SIGTERM signal to process group
[2023-05-17T12:57:33.445+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/hooks/subprocess.py", line 91, in run_command
    for raw_line in iter(self.sub_process.stdout.readline, b""):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 1542, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-05-17T12:57:33.453+0200] {taskinstance.py:1368} INFO - Marking task as FAILED. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230516T000000, start_date=20230517T105331, end_date=20230517T105733
[2023-05-17T12:57:33.469+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1802 for task extract_data ((psycopg2.errors.ForeignKeyViolation) insert or update on table "task_fail" violates foreign key constraint "task_fail_ti_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(etl_news_pipeline, extract_data, scheduled__2023-05-16T00:00:00+00:00, -1) is not present in table "task_instance".

[SQL: INSERT INTO task_fail (task_id, dag_id, run_id, map_index, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(run_id)s, %(map_index)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 'extract_data', 'dag_id': 'etl_news_pipeline', 'run_id': 'scheduled__2023-05-16T00:00:00+00:00', 'map_index': -1, 'start_date': datetime.datetime(2023, 5, 17, 10, 53, 31, 430011, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2023, 5, 17, 10, 57, 33, 451258, tzinfo=Timezone('UTC')), 'duration': 242}]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 2387)
[2023-05-17T12:57:33.551+0200] {process_utils.py:79} INFO - Process psutil.Process(pid=2395, status='terminated', started='12:53:31') (2395) terminated with exit code None
[2023-05-17T12:57:33.552+0200] {process_utils.py:79} INFO - Process psutil.Process(pid=2387, status='terminated', exitcode=1, started='12:53:31') (2387) terminated with exit code 1
[2023-05-17T13:01:39.497+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:01:39.503+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:01:39.504+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T13:01:39.511+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T13:01:39.528+0200] {standard_task_runner.py:57} INFO - Started process 2816 to run task
[2023-05-17T13:01:39.536+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1806', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmp5d9g7pk5']
[2023-05-17T13:01:39.538+0200] {standard_task_runner.py:85} INFO - Job 1806: Subtask extract_data
[2023-05-17T13:01:39.598+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T13:01:39.680+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T13:01:39.681+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T13:01:39.682+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T13:01:39.702+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T13:41:33.352+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:41:33.359+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:41:33.360+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T13:41:33.370+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T13:41:33.390+0200] {standard_task_runner.py:57} INFO - Started process 4424 to run task
[2023-05-17T13:41:33.398+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1887', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmplxru7k6h']
[2023-05-17T13:41:33.401+0200] {standard_task_runner.py:85} INFO - Job 1887: Subtask extract_data
[2023-05-17T13:41:33.459+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T13:41:33.526+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T13:41:33.527+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T13:41:33.528+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T13:41:33.549+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T13:42:40.077+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T13:42:40.104+0200] {subprocess.py:93} INFO - 0 2023-05-17 11:40:29  ...  Hunt says the government will open borders for...
[2023-05-17T13:42:40.104+0200] {subprocess.py:93} INFO - 
[2023-05-17T13:42:40.106+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T13:42:40.742+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-17T13:42:40.841+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230516T000000, start_date=20230517T114133, end_date=20230517T114240
[2023-05-17T13:42:40.908+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T13:42:41.018+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-17T13:53:05.792+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:53:05.799+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:53:05.800+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T13:53:05.813+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T13:53:05.834+0200] {standard_task_runner.py:57} INFO - Started process 5846 to run task
[2023-05-17T13:53:05.842+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1911', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmps68ce4or']
[2023-05-17T13:53:05.845+0200] {standard_task_runner.py:85} INFO - Job 1911: Subtask extract_data
[2023-05-17T13:53:05.910+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T13:53:05.967+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T13:53:05.969+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T13:53:05.970+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T13:53:05.992+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T13:53:47.991+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T13:53:47.993+0200] {subprocess.py:93} INFO - 0 2023-05-17 11:41:54  ...  Hunt says the government will open borders for...
[2023-05-17T13:53:47.994+0200] {subprocess.py:93} INFO - 
[2023-05-17T13:53:47.994+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T13:53:48.481+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-17T13:53:48.544+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230516T000000, start_date=20230517T115305, end_date=20230517T115348
[2023-05-17T13:53:48.568+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T13:53:48.608+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2023-05-17T13:57:55.021+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:57:55.031+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T13:57:55.031+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T13:57:55.044+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T13:57:55.066+0200] {standard_task_runner.py:57} INFO - Started process 6434 to run task
[2023-05-17T13:57:55.075+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1919', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpmki_sndh']
[2023-05-17T13:57:55.078+0200] {standard_task_runner.py:85} INFO - Job 1919: Subtask extract_data
[2023-05-17T13:57:55.146+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T13:57:55.210+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T13:57:55.211+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T13:57:55.212+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T13:57:55.238+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T13:58:40.445+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T13:58:40.448+0200] {subprocess.py:93} INFO - 0 2023-05-17 11:57:36  ...  Chancellor says care homes and construction ar...
[2023-05-17T13:58:40.448+0200] {subprocess.py:93} INFO - 
[2023-05-17T13:58:40.449+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T13:58:40.854+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-17T13:58:40.891+0200] {taskinstance.py:1847} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.ForeignKeyViolation: insert or update on table "xcom" violates foreign key constraint "xcom_task_instance_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(etl_news_pipeline, extract_data, scheduled__2023-05-16T00:00:00+00:00, -1) is not present in table "task_instance".


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 2377, in xcom_push
    XCom.set(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/airflow/models/xcom.py", line 264, in set
    session.flush()
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 3449, in flush
    self._flush(objects)
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 3588, in _flush
    with util.safe_reraise():
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/orm/session.py", line 3549, in _flush
    flush_context.execute()
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py", line 456, in execute
    rec.execute(self)
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py", line 245, in save_obj
    _emit_insert_statements(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py", line 1097, in _emit_insert_statements
    c = connection._execute_20(
        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/sql/elements.py", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1948, in _execute_context
    self._handle_dbapi_exception(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 2129, in _handle_dbapi_exception
    util.raise_(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/engine/base.py", line 1905, in _execute_context
    self.dialect.do_execute(
  File "/Users/niyantmehta/opt/anaconda3/envs/finalvenv/lib/python3.11/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.IntegrityError: (psycopg2.errors.ForeignKeyViolation) insert or update on table "xcom" violates foreign key constraint "xcom_task_instance_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(etl_news_pipeline, extract_data, scheduled__2023-05-16T00:00:00+00:00, -1) is not present in table "task_instance".

[SQL: INSERT INTO xcom (dag_run_id, task_id, map_index, key, dag_id, run_id, value, timestamp) VALUES (%(dag_run_id)s, %(task_id)s, %(map_index)s, %(key)s, %(dag_id)s, %(run_id)s, %(value)s, %(timestamp)s)]
[parameters: {'dag_run_id': 1051, 'task_id': 'extract_data', 'map_index': -1, 'key': 'return_value', 'dag_id': 'etl_news_pipeline', 'run_id': 'scheduled__2023-05-16T00:00:00+00:00', 'value': <psycopg2.extensions.Binary object at 0x16514ed90>, 'timestamp': datetime.datetime(2023, 5, 17, 11, 58, 40, 885800, tzinfo=Timezone('UTC'))}]
(Background on this error at: https://sqlalche.me/e/14/gkpj)
[2023-05-17T13:58:40.948+0200] {taskinstance.py:1368} INFO - Marking task as FAILED. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230516T000000, start_date=20230517T115755, end_date=20230517T115840
[2023-05-17T13:58:40.963+0200] {standard_task_runner.py:104} ERROR - Failed to execute job 1919 for task extract_data ((psycopg2.errors.ForeignKeyViolation) insert or update on table "task_fail" violates foreign key constraint "task_fail_ti_fkey"
DETAIL:  Key (dag_id, task_id, run_id, map_index)=(etl_news_pipeline, extract_data, scheduled__2023-05-16T00:00:00+00:00, -1) is not present in table "task_instance".

[SQL: INSERT INTO task_fail (task_id, dag_id, run_id, map_index, start_date, end_date, duration) VALUES (%(task_id)s, %(dag_id)s, %(run_id)s, %(map_index)s, %(start_date)s, %(end_date)s, %(duration)s) RETURNING task_fail.id]
[parameters: {'task_id': 'extract_data', 'dag_id': 'etl_news_pipeline', 'run_id': 'scheduled__2023-05-16T00:00:00+00:00', 'map_index': -1, 'start_date': datetime.datetime(2023, 5, 17, 11, 57, 55, 22110, tzinfo=Timezone('UTC')), 'end_date': datetime.datetime(2023, 5, 17, 11, 58, 40, 946525, tzinfo=Timezone('UTC')), 'duration': 45}]
(Background on this error at: https://sqlalche.me/e/14/gkpj); 6434)
[2023-05-17T13:58:40.999+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 1
[2023-05-17T13:58:41.031+0200] {taskinstance.py:2674} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-17T14:00:26.121+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T14:00:26.128+0200] {taskinstance.py:1125} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [queued]>
[2023-05-17T14:00:26.128+0200] {taskinstance.py:1331} INFO - Starting attempt 1 of 2
[2023-05-17T14:00:26.143+0200] {taskinstance.py:1350} INFO - Executing <Task(BashOperator): extract_data> on 2023-05-16 00:00:00+00:00
[2023-05-17T14:00:26.161+0200] {standard_task_runner.py:57} INFO - Started process 6729 to run task
[2023-05-17T14:00:26.169+0200] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'etl_news_pipeline', 'extract_data', 'scheduled__2023-05-16T00:00:00+00:00', '--job-id', '1923', '--raw', '--subdir', 'DAGS_FOLDER/etl_pipeline.py', '--cfg-path', '/var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T/tmpq5qa5bxg']
[2023-05-17T14:00:26.171+0200] {standard_task_runner.py:85} INFO - Job 1923: Subtask extract_data
[2023-05-17T14:00:26.231+0200] {task_command.py:410} INFO - Running <TaskInstance: etl_news_pipeline.extract_data scheduled__2023-05-16T00:00:00+00:00 [running]> on host niyants-mbp.fritz.box
[2023-05-17T14:00:26.278+0200] {taskinstance.py:1568} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_news_pipeline' AIRFLOW_CTX_TASK_ID='extract_data' AIRFLOW_CTX_EXECUTION_DATE='2023-05-16T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-05-16T00:00:00+00:00'
[2023-05-17T14:00:26.279+0200] {subprocess.py:63} INFO - Tmp dir root location: 
 /var/folders/qv/qkzfd8cs7hdftb0synd9cw3h0000gn/T
[2023-05-17T14:00:26.280+0200] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'python /Users/niyantmehta/Spiced/ETL_Pipeline/airflow/ETL_dags/extract.py extract_data']
[2023-05-17T14:00:26.302+0200] {subprocess.py:86} INFO - Output:
[2023-05-17T14:01:08.086+0200] {subprocess.py:93} INFO -                  date  ...                                            summary
[2023-05-17T14:01:08.088+0200] {subprocess.py:93} INFO - 0 2023-05-17 11:57:36  ...  Chancellor says care homes and construction ar...
[2023-05-17T14:01:08.089+0200] {subprocess.py:93} INFO - 
[2023-05-17T14:01:08.089+0200] {subprocess.py:93} INFO - [1 rows x 5 columns]
[2023-05-17T14:01:08.528+0200] {subprocess.py:97} INFO - Command exited with return code 0
[2023-05-17T14:01:08.579+0200] {taskinstance.py:1368} INFO - Marking task as SUCCESS. dag_id=etl_news_pipeline, task_id=extract_data, execution_date=20230516T000000, start_date=20230517T120026, end_date=20230517T120108
[2023-05-17T14:01:08.639+0200] {local_task_job_runner.py:232} INFO - Task exited with return code 0
[2023-05-17T14:01:08.676+0200] {taskinstance.py:2674} INFO - 1 downstream tasks scheduled from follow-on schedule check
